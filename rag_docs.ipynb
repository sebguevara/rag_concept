{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_chroma import Chroma\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"documents_db\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_docs_db\",  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and chunk contents of the blog\n",
    "PDF_PATH = \"data/rag.pdf\"\n",
    "\n",
    "pdf_loader = PyPDFLoader(PDF_PATH)\n",
    "docs = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "MAX_BATCH = 5461\n",
    "\n",
    "# Index en lotes\n",
    "for batch in chunks(all_splits, MAX_BATCH):\n",
    "    vector_store.add_documents(documents=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\johan\\Documents\\code\\ia_ps\\.venv\\Lib\\site-packages\\langsmith\\client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define prompt for question-answering\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG significa \"Retrieval-Augmented Generation\", que es un modelo que combina enfoques de generación de texto y recuperación de documentos. Este enfoque permite a los modelos generar respuestas más precisas y diversas al usar documentos relevantes como contexto adicional. RAG se destaca por lograr un rendimiento de vanguardia sin requerir entrenamiento de preprocesamiento costoso y especializado.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"Qué significa RAG?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La tecnología RAG fue desarrollada por investigadores asociados con Hugging Face, en colaboración con conceptos y arquitecturas como el Dense Passage Retriever (DPR) y el modelo BART. RAG combina la generación flexible y el rendimiento de enfoques de recuperación, ofreciendo resultados competitivos sin la necesidad de preentrenamientos costosos. Se basa en técnicas previas de memoria no paramétrica pero fue optimizada para ser eficaz en tareas de generación y recuperación de información.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"Quienes crearon la tecnologia RAG?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La tecnología RAG (Retrieval-Augmented Generation) combina la generación de respuestas basada en modelos paramétricos y el uso de documentos recuperados como contexto adicional. Utiliza un recuperador para obtener pasajes relevantes en función de una consulta y luego genera respuestas considerando tanto la entrada como estos documentos recuperados. RAG muestra un rendimiento de vanguardia sin la necesidad de componentes adicionales como un lector extractivo o re-ranker.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"En qué consiste la tecnologia RAG?\"})\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
